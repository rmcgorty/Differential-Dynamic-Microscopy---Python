{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll try analyzing Sylas's data of beads in networks using methods described in this [Cho et al 2020 PRL paper](https://link.aps.org/doi/10.1103/PhysRevLett.124.088005).\n",
    "\n",
    "With DDM, we take a movie and generate the DDM matrix, $D(q,\\Delta t)$ (the generation of this DDM matrix should already have been done before going through this code). This can be fit to the function:\n",
    "$D(q,\\Delta t) = A(q)(1 - f(q,\\Delta t)) + B(q)$. \n",
    "The function $f(q,\\Delta t)$ is called the intermediate scattering function (ISF). And we usually assume it has the form: $f(q,\\Delta t) = \\mbox{exp}(-\\Delta t / \\tau (q))^{\\alpha (q)}$ where $\\alpha (q)$ is the stretching exponent and $\\tau (q)$ is the characteristic decay time.\n",
    "\n",
    "What we have usually tried is to take $D(q,\\Delta t)$ and, for each wave vector $q$, fit it to find the parameters $A, B, \\tau, \\text{ and } \\alpha$.\n",
    "\n",
    "What we do now (using the methods of [Cho et al](https://link.aps.org/doi/10.1103/PhysRevLett.124.088005)) is get the parameters $A$ and $B$ from the images themselves. Then we can get the ISF: $f(q,\\Delta t) = 1 - \\frac{D(q,\\Delta t) - B(q)}{A(q)}.$ \n",
    "\n",
    "We also add a new paramter to the ISF: the non-ergodicity parameter, $C$. So now we have that the ISF is equal to: $f(q,\\Delta t) = (1-C(q))\\mbox{exp}(-\\Delta t / \\tau (q))^{\\alpha (q)} + C(q)$. If $C$ is zero, then this ISF is just $\\mbox{exp}(-\\Delta t / \\tau (q))^{\\alpha (q)}$, as we had before. And that's the expected case for ergodic dynamics. But if the system is non-ergodic, then we expect a non-zero $C$, somewhere between 0 and 1. \n",
    "\n",
    "Note that in this code, we refer to $D(q, \\Delta t)$ as 'ravs'. That is because getting $D(q, \\Delta t)$ invovles finding the <b>r</b>adial <b>av</b>erages of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiff_file.py:1995: UserWarning: failed to import _tifffile.decodepackbits\n",
      "  warnings.warn(\"failed to import %s\" % module_function)\n",
      "tiff_file.py:1995: UserWarning: failed to import _tifffile.decodelzw\n",
      "  warnings.warn(\"failed to import %s\" % module_function)\n",
      "tiff_file.py:1995: UserWarning: failed to import _tifffile.unpackints\n",
      "  warnings.warn(\"failed to import %s\" % module_function)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "font_plt = {'family': 'serif','color':  'darkred','weight': 'normal','size': 8,}\n",
    "font_plt_ax = {'family': 'serif','color':  'black','weight': 'normal', 'size': 8,}\n",
    "\n",
    "import numpy as np #numerical python used for working with arrays, mathematical operations\n",
    "import time #useful for timing functions\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import glob #glob is helpful for searching for filenames or directories\n",
    "import ddm_clean as ddm #this is the module containing the differential dynamic microscopy code\n",
    "import scipy #scientific python\n",
    "from scipy.signal import blackmanharris as bh #for Blackman-Harris windowing\n",
    "from scipy.optimize import leastsq\n",
    "import pickle #for saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ddm_clean' from 'ddm_clean.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ddm) #reload of ddm necessary if changes have been made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie number and ROI specified below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#Specify where the data is stored and the image data filename. Must be in tiff format\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "RMdate = '2020-09-23'\n",
    "JGdate = '20200923'\n",
    "#######################\n",
    "condition=\"ActinMyosin\"\n",
    "#condition=\"AMT_myosin\"\n",
    "#condition=\"100M_myosin\"\n",
    "#condition=\"75A25M_myosin\"\n",
    "#condition=\"25A75M_myosin\"\n",
    "########################\n",
    "# Select Movie Number  #\n",
    "movie_num = 1\n",
    "########################\n",
    "########### Select ROI here ###############\n",
    "ROI = 0  # <---- select ROI (0,256,512, or 768)\n",
    "###########################################\n",
    "\n",
    "data_dir = \"Y:\\\\Jon_Garamella\\\\data\\\\active_networks\\\\videos_date\\\\%s\\\\20200922_%s_594beads_25X_561OD3_50msEXPO_20fps_%s\\\\\" % (RMdate,condition,movie_num)\n",
    "data_file = \"%s_AMT_594beads_25X_561OD3_50msEXPO_20fps_%s_MMStack_Pos0.ome.tif\" %(JGdate,movie_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to look at the images, uncomment this out, otherwise we won't need to load the images or generate the ffts to get A&B as it's already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiff_file.py:725: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for p in pages)\n"
     ]
    }
   ],
   "source": [
    "#Image read using tiff_file module\n",
    "#im = ddm.tiff_file.imread(data_dir+data_file)\n",
    "\n",
    "#Display the second frame of the image\n",
    "#plt.figure(figsize=(8,3))\n",
    "#plt.matshow(im[1], cmap=matplotlib.cm.gray, fignum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open up the previosly generated DDM matrix data and A&B data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#  Specify where the DDM matrix data is stored\n",
    "#######################################################################################\n",
    "data_dir = \"Y:\\\\Jon_Garamella\\\\data\\\\active_networks\\\\videos_date\\\\%s\\\\ddm_analysis\\\\\" %(RMdate,condition)\n",
    "rav_file = \"%s_%s_beads_405stim_561OD3_100ms_10fps_%s_%s_256x256_FFTDIFFS_dts_ravs.p\" %(JGDate,condition,movie_num,ROI)\n",
    "\n",
    "f = open(data_dir + rav_file,'rb')\n",
    "p_data = pickle.load(f)\n",
    "f.close()\n",
    "#copy over that data stored in the dictionary\n",
    "ravs = p_data['ravs']\n",
    "dts = p_data['dts']\n",
    "print(\"The 'keys' contained in this pickle'd dictionary are: \", p_data.keys())\n",
    "\n",
    "#Copy over the Amplitude and Background data generated previously\n",
    "\n",
    "data_file = rav_file[:-19]+\"imageffts_for_AB.p\"\n",
    "f = open(data_dir + data_file,'rb')\n",
    "p_data = pickle.load(f)\n",
    "f.close()\n",
    "rad_av_av_fftsq = p_data['rad_av_av_fftsq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Specify the frame rate (fps) and pixel size\n",
    "##########################################################################\n",
    "\n",
    "fps = 10.0 #The frame rate the video data was recorded at. \n",
    "times = dts/fps #Create the list of delay times in units of seconds\n",
    "pixel_size = 0.194 #pixel size in microns\n",
    "numPixels = 256 #number pof pixels in ROI\n",
    "q = np.arange(0,numPixels/2)*2*np.pi*(1./(numPixels*pixel_size)) #Convert the spatial frequencies to wave vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here, we'll show the image structure function for a particular q-value. \n",
    "\n",
    "qv=-1 # <-- this is the last q-value. useful for getting estimate of background\n",
    "fig = plt.figure(figsize=(6,6./1.618))\n",
    "plt.semilogx(times, ravs[:,qv],'ro')\n",
    "ax = plt.gca()\n",
    "plt.xlabel('Time (s)', fontdict=font_plt_ax, labelpad=-3);\n",
    "plt.title(\"D(q,dt) for q of %.2f $\\mu$m$^{-1}$\" % q[qv]);\n",
    "plt.hlines(ravs[0,qv], times[0], times[-1], linestyles='dashed')\n",
    "print(\"Horizontal line at %.1f\" % ravs[0,qv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we could try fitting $D(q, \\Delta t)$ to the model described at the very beginning to determine $A$, $B$, $\\tau$ and $\\alpha$. If you want to do that, you'll find the code at the end of this notebook. But that step isn't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(q[3:], rad_av_av_fftsq[0,2:],'ro')\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"0.5 * (A+B)\")\n",
    "plt.hlines(rad_av_av_fftsq[0,-1], q[3], q[-1], linestyles='dashed')\n",
    "plt.title(\"Dashed horizontal line at %.2f\" % rad_av_av_fftsq[0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of the DDM data, I'd say the background is ~ 40 (in many cases, depends on video). \n",
    "Based off the above plot, seems like it (1/2)(A+B) is plateauing at high q to around ??. So B is around twice that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#You can play around with this 'background' parameter\n",
    "############################################################\n",
    "background = 720\n",
    "new_amplitude = (2*rad_av_av_fftsq[0]) - background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(q[3:], new_amplitude[2:], 'ro', label='Amplitude from new method')\n",
    "try: plt.plot(q[3:], amp[2:], 'mo', label='Amplitude from fitting method')\n",
    "except: print(\"'amp' not defined\")\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"amplitude\")\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our new value for amplitude, let's find the intermediate scattering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just double checking sizes of arrays:\n",
    "print(\"size of ravs array (the ddm matrix or image struct func): %i by %i\" % ravs.shape)\n",
    "print(\"size of new amplitudes: %i\" % new_amplitude.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize arrays to store the ISF (intermediate scattering function)\n",
    "isf = np.zeros_like(ravs) #decay time\n",
    "\n",
    "for i in range(1,ravs.shape[1]):\n",
    "    isf[:,i] = 1 - ((ravs[:,i] - background) / new_amplitude[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10)) #Create figure of size 15x15 (inches)\n",
    "\n",
    "\n",
    "#Loop over 8 different q-values to plot the ISF\n",
    "for i,q_index in enumerate([5,10,15,20,25,30,35,45]):\n",
    "\n",
    "    ax = plt.subplot(4,2,i+1) #creating 4 subplots in a 2x2 grid\n",
    "    ax.semilogx(times,isf[:,q_index],'ro',alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel(\"Time (s)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "    ax.set_title(\"Fit for q-index of %i. So q = %.3f 1/$\\mu$m\" % (q_index, q[q_index]), fontdict=font_plt_ax)\n",
    "    \n",
    "    ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We'll start fitting the ISF with a streched exponential if the video is not stimulated and a compressed exponential if the video is stimulated\n",
    "\n",
    "The fitting function, `isf_fitting`, is described below. After running through this cell (and the next 3 or 4), you'll need to come back to this point and re-run this function after you settle on a stretching exponent to use. You'll come back to the first uncommented-out line: <br>\n",
    "`STRETCHING_EXP = 0.6` <br>\n",
    "and set the value (which is the stretching exponent, $\\alpha (q)$) to the correct value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# Stretching exponent.\n",
    "# This number must be between 0 and 1. \n",
    "# And it will probably be around 0.6.\n",
    "# When you specify the value, don't go beyond hundredths place.\n",
    "# Set this to the AVERAGE VALUE you found when this parameter was allowed to vary.\n",
    "##################################################################################################\n",
    "STRETCHING_EXP = 1.0  # <----- THIS NUMBER NEEDS TO CHANGE AFTER GOING THROUGH NEXT COUPLE STEPS!\n",
    "\n",
    "##################################################################################################\n",
    "# Fix stretching exponent... it may help to fix\n",
    "# Set this to true or false. \n",
    "# \n",
    "# BUT WE'LL SET THIS PARAMETER'S VALUE (TRUE or FALSE) ELSEWHERE\n",
    "# So leave this commented out for now\n",
    "#  \n",
    "##################################################################################################\n",
    "FIX_STRETCHING_EXP = False #False will allow stretching exponent to vary. True will hold constant\n",
    "\n",
    "def errorfunc_for_scipy_leastsq_fit(params, data, times):\n",
    "    theory = ddm.dTheoryNonErgISF(times, params[0],params[1],params[2])\n",
    "    return data-theory\n",
    "\n",
    "def isf_fitting(data, times):\n",
    "    '''\n",
    "    This function does the ISF fitting. \n",
    "\n",
    "    We'll just use one round. Using Levenberg-Marquardt method with the mpfit module. \n",
    "    '''\n",
    "    \n",
    "    #Our parameters are: c (non-ergodicity param), tau, stretching epxonent\n",
    "    pars = np.zeros(3)*1.0\n",
    "    minp = np.zeros_like(pars)\n",
    "    maxp = np.zeros_like(pars)\n",
    "    lmin = np.array([True, True, True])\n",
    "    lmax = np.array([True, True, True])\n",
    "    fix = np.array([False, False, False])\n",
    "    \n",
    "    #come up with limits:\n",
    "    minp[0] = 0.0 #minimum non-erg parameter\n",
    "    maxp[0] = 1.0 #maximum non-erg parameter\n",
    "    minp[1] = 0.01 #minimum decay time\n",
    "    maxp[1] = 3000.0  #maximum decay time\n",
    "    minp[2] = 0.1 #minimum stretching exponent\n",
    "    maxp[2] = 1.0 #maximum stretching exponent\n",
    "    \n",
    "    #initial guesses \n",
    "    pars[0] = 0.25 #non-ergodicity parameter\n",
    "    pars[1] = 50.0 #decay time\n",
    "    ############################################################################\n",
    "    # Below (pars[2]) is the stretching exponent. \n",
    "    # After letting it vary, set to average value and\n",
    "    # fix it at that.\n",
    "    #\n",
    "    # YOU MUST CHANGE THIS TO AVERAGE VALUE OVER REASONALBE RANGE OF Q\n",
    "    ###########################################################################\n",
    "    pars[2] = STRETCHING_EXP #stretching exponent. \n",
    "    \n",
    "    fix[2] = FIX_STRETCHING_EXP   #True or False -- set above\n",
    "    \n",
    "    # First step, use the Scipy Least Squares function to find best parameters\n",
    "    #   We will then use those parameters as initial guess in the Levenberg-Marquardt method\n",
    "    fitparams_lstsq_temp = leastsq(errorfunc_for_scipy_leastsq_fit, pars, args=(data,times))\n",
    "    fitparams_isf_lstsq = fitparams_lstsq_temp[0]\n",
    "    theory_isf_lstsq = ddm.dTheoryNonErgISF(times, fitparams_isf_lstsq[0], fitparams_isf_lstsq[1], fitparams_isf_lstsq[2])\n",
    "    \n",
    "    #sometimes the leastsq's function will return parameters outside the limits we impose\n",
    "    #  so check for that and correct if necessary\n",
    "    for i in [1,2]:\n",
    "        if not fix[i]:\n",
    "            if fitparams_isf_lstsq[i] > maxp[i]:\n",
    "                pars[i] = 0.99*maxp[i]\n",
    "            elif fitparams_isf_lstsq[i] < minp[i]:\n",
    "                pars[i] = 1.01*minp[i]\n",
    "            else:\n",
    "                pars[i] = fitparams_isf_lstsq[i]\n",
    "        if fitparams_isf_lstsq[0]>0:\n",
    "            if fitparams_isf_lstsq[0]<1.0:\n",
    "                pars[0] = fitparams_isf_lstsq[0]\n",
    "\n",
    "\n",
    "    fitparams_isf, theory_isf, errCode, chi2 = ddm.newFit_ISF(data,times,pars,minp,maxp,lmin,lmax,fix,\n",
    "                                                              logfit=False,quiet=True,factor=1)\n",
    "    \n",
    "    return fitparams_isf, theory_isf, chi2, fitparams_isf_lstsq, theory_isf_lstsq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we define the time lag that we end the fits at. We do this because the data for long time lags gets noisier. It also gets more noisy for high q at long times than low q at long times. So we make the last time we fit to a function of q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You'll need to change that first number in the linspace function if you use the variable times\n",
    "last_times = np.linspace(400,2,num=len(q),dtype=np.int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we'll inspect some (8) of the fits to the ISF. Hopefully they look okay. Make a note if any of them look off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# We don't have to fit all time lags. The long time lags may be noisy.\n",
    "#########################################################################\n",
    "# not implemented anymore: last_time = -350 # ONLY FIT UP TO THIS FINAL TIME POINT\n",
    "\n",
    "plt.figure(figsize=(10,12)) #Create figure of size 10x12\n",
    "\n",
    "#########################################################################\n",
    "# In making these plots, we'll not fix the stretching exponent.\n",
    "# But feel free to change this.\n",
    "#########################################################################\n",
    "FIX_STRETCHING_EXP = False\n",
    "\n",
    "\n",
    "#Loop over 8 different q-values to plot the ISF\n",
    "for i,q_index in enumerate([15,20,25,30,35,40,50,60]):\n",
    "    \n",
    "    last_time = last_times[q_index]\n",
    "    #last_time = 300\n",
    "    fp_isf, theory_isf, chi2, fp_isf_lstsq, theory_isf_lstsq = isf_fitting(isf[:last_time,q_index],times[:last_time])\n",
    "    full_time_theory = ddm.dTheoryNonErgISF(times, *fp_isf)\n",
    "\n",
    "    ax = plt.subplot(4,2,i+1) #creating 4 subplots in a 2x2 grid\n",
    "    ax.semilogx(times[:],isf[:,q_index],'ro',alpha=0.8)\n",
    "    ax.plot(times[:last_time], theory_isf, '-b',lw=3,alpha=0.5) #BLUE LINE: Leven-Marq fitting method\n",
    "    ax.plot(times, full_time_theory, '--k',lw=1,alpha=0.8) #BLUE LINE: Leven-Marq fitting method\n",
    "    ax.plot(times[:last_time], theory_isf_lstsq,'-g',lw=3,alpha=0.5) #GREEN LINE: scipy.optimize's leastsquares function\n",
    "    \n",
    "    ax.text(0.15,0.4, \"decay time: %.1f, %.1f\" % (fp_isf[1], fp_isf_lstsq[1]), fontdict=font_plt_ax)\n",
    "    ax.text(0.15,0.3, \"non-erg param: %.2f, %.2f\" % (fp_isf[0], fp_isf_lstsq[0]), fontdict=font_plt_ax)\n",
    "    ax.text(0.15,0.2, \"stretch exp: %.2f, %.2f\" % (fp_isf[2], fp_isf_lstsq[2]), fontdict=font_plt_ax)\n",
    "    \n",
    "    \n",
    "    ax.set_xlabel(\"Time (s)\", fontdict=font_plt_ax)\n",
    "    ax.set_title(\"Fit for q-index of %i. So q = %.3f 1/$\\mu$m\" % (q_index, q[q_index]), fontdict=font_plt_ax)\n",
    "    ax.set_ylim(-0.05,1.05)\n",
    "    \n",
    "plt.savefig(data_dir+data_file[:-18]+\"ISF_fits_ExpoFit.png\",dpi=150)\n",
    "print(\"Saved as %s\" % data_dir+data_file[:-18]+\"ISF_fits_ExpoFit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the above fits to the normalize image structure function look okay. If not, you can try adjusting the `last_times` parameter. Sometimes, removing more of the last few time points from the data we fit to helps since the data associated with very long time lags tends to be noisier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we do the fits for each wave vector (each q). We do this twice. One with fixing the stretching exponent and one time with letting it vary. When you see the results after the following code block, you'll choose the value for this stretching exponent and insert that value back into the block of code where the function `isf_fitting` was defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we'll do that fit for *all* q-values\n",
    "start=timer()\n",
    "\n",
    "FIX_STRETCHING_EXP = True\n",
    "#Initialize arrays to store the fit paramters\n",
    "tau_v2 = np.zeros_like(ravs[0,:]) #decay time\n",
    "c = np.zeros_like(tau_v2) #this is the non-ergodicity parameter\n",
    "alph_v2 = np.zeros_like(tau_v2) #alpha (stretching exponent)\n",
    "for i in range(1,len(tau_v2)):\n",
    "    last_time = last_times[i]\n",
    "    fp_isf, theory_isf, chi2, fp_isf_lstsq, theory_isf_lstsq = isf_fitting(isf[:last_time,i],times[:last_time])\n",
    "    c[i] = fp_isf[0]\n",
    "    tau_v2[i] = fp_isf[1]\n",
    "    alph_v2[i] = fp_isf[2]\n",
    "\n",
    "    \n",
    "FIX_STRETCHING_EXP = False\n",
    "#Initialize arrays to store the fit paramters -- THIS TIME FIXING ALPHA\n",
    "tau_v2_varyalpha = np.zeros_like(ravs[0,:]) #decay time\n",
    "c_varyalpha = np.zeros_like(tau_v2) #this is the non-ergodicity parameter\n",
    "alph_v2_varyalpha = np.zeros_like(tau_v2) #alpha (stretching exponent)\n",
    "for i in range(1,len(tau_v2_varyalpha)):\n",
    "    last_time = last_times[i]\n",
    "    fp_isf, theory_isf, chi2, fp_isf_lstsq, theory_isf_lstsq = isf_fitting(isf[:last_time,i],times[:last_time])\n",
    "    c_varyalpha[i] = fp_isf[0]\n",
    "    tau_v2_varyalpha[i] = fp_isf[1]\n",
    "    alph_v2_varyalpha[i] = fp_isf[2]\n",
    "\n",
    "end=timer()\n",
    "print 'seconds elapsed: %.2f' %(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot the decay time versus the wave vector\n",
    "fig = plt.figure(figsize=(8,3*8/1.618)); ax = fig.gca();\n",
    "ax = plt.subplot(3,1,1)\n",
    "\n",
    "plt.title(\"Tau vs q -- \" + data_dir.split('\\\\')[-4] + \"; Movie \" + str(movie_num) + \"; ROI \" + str(ROI))\n",
    "qs=q[3:-1]\n",
    "new_taus=ddm.newt(tau_v2[2:-1],alph_v2[2:-1])\n",
    "new_taus_varyalpha=ddm.newt(tau_v2_varyalpha[2:-1],alph_v2_varyalpha[2:-1])\n",
    "#ax.loglog(qs, tau_v2[2:-1],'g.',alpha=0.2) \n",
    "ax.loglog(qs, new_taus,'bo', label='with str exponent fixed')\n",
    "ax.loglog(qs, new_taus_varyalpha,'gs',alpha=0.3, label='allowing str exp to vary')\n",
    "\n",
    "#############################################################################\n",
    "# Pick the range of q-value that seem to fit best (usually ~25ish to ~42ish)\n",
    "#############################################################################\n",
    "minq=13\n",
    "maxq=35\n",
    "\n",
    "#######################################\n",
    "#  You can comment out a large block  #\n",
    "#  by highlighting it and using three #\n",
    "#  quotation marks '''                #\n",
    "#######################################\n",
    "\n",
    "#############################################################################\n",
    "##################### Fit with alpha fixed!!!!!!! ###########################\n",
    "#############################################################################\n",
    "qmin=qs[minq]\n",
    "qmax=qs[maxq]\n",
    "b = np.where((qs>=qmin)&(qs<=qmax))\n",
    "ax.plot(qs[b[0]],new_taus[b[0]],'r+',label='good q range')\n",
    "a = np.polyfit(np.log(qs[b[0]]),np.log(new_taus[b[0]]), 1)\n",
    "slope = a[0]\n",
    "coef1 = np.exp(a[1])\n",
    "alpha = 2./(-1*slope)\n",
    "Dif = (1.0/coef1)**alpha\n",
    "tau_fit = coef1*(qs**(-2.0/alpha))\n",
    "ax.plot(qs, tau_fit, '-k')\n",
    "\n",
    "#############################################################################\n",
    "##################### Fit with alpha varied!!!!!!! ###########################\n",
    "#############################################################################\n",
    "'''qmin=qs[minq]\n",
    "qmax=qs[maxq]\n",
    "b = np.where((qs>=qmin)&(qs<=qmax))\n",
    "ax.plot(qs[b[0]],new_taus_varyalpha[b[0]],'r+',label='good q range')\n",
    "a = np.polyfit(np.log(qs[b[0]]),np.log(new_taus_varyalpha[b[0]]), 1)\n",
    "slope = a[0]\n",
    "coef1 = np.exp(a[1])\n",
    "alpha = 2./(-1*slope)\n",
    "Dif = (1.0/coef1)**alpha\n",
    "tau_fit = coef1*(qs**(-2.0/alpha))\n",
    "ax.plot(qs, tau_fit, '-k')'''\n",
    "\n",
    "## does it fit ballistically?\n",
    "fix_speed = .01\n",
    "fix_slope = 2. #diffusive = 2, ballistic 1\n",
    "ax.plot(qs[:-20], (1./fix_speed) * (1./(qs[:-20]**(fix_slope))), '--m', label=\"diffusive, $\\ t$$^{-2}$\")\n",
    "\n",
    "ax.text(0.4,1, \"alpha: %.4f\" % alpha)\n",
    "ax.text(1,2, \"k (if subdiff.): %.4f\" % Dif)\n",
    "ax.text(0.4,4, \"%i < q < %i\" % (minq, maxq))\n",
    "ax.text(0.4,2, \"slope: %.4f\" % (slope))\n",
    "ax.text(1,1, \"speed (if ball.): %.4f\" % (fix_speed))\n",
    "        \n",
    "        \n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"tau (s)\", fontdict=font_plt_ax)\n",
    "ax.set_ylim(0.7,1000)\n",
    "ax.legend(loc=1)\n",
    "\n",
    "\n",
    "#Plot the non-erg parameter versus the wave vector\n",
    "ax = plt.subplot(3,1,2)\n",
    "plt.title(\"Non-ergodicity (fixed background to %.1f)\" % background)\n",
    "ax.semilogx(q[3:-1], c[2:-1], 'ro')\n",
    "ax.semilogx(q[3:-1], c_varyalpha[2:-1], 'rs', alpha=0.4)\n",
    "c_range = np.where((qs>=q[45])&(qs<=q[75]))\n",
    "ax.semilogx(qs[c_range], c[2:-1][c_range], 'k.',label='region to find local max')\n",
    "c_local_max = c[2:-1][c_range].max()\n",
    "where_local_max = np.argmax(c[2:-1][c_range])\n",
    "ax.semilogx(qs[c_range][where_local_max], c_local_max, '*', c='y', ms=10, label='local max: %.2f' % c_local_max)\n",
    "c_range = np.where((qs>=q[25])&(qs<=q[50]))\n",
    "ax.semilogx(qs[c_range], c[2:-1][c_range], 'b.',label='region to find local min')\n",
    "c_local_min = c[2:-1][c_range].min()\n",
    "where_local_min = np.argmin(c[2:-1][c_range])\n",
    "ax.semilogx(qs[c_range][where_local_min], c_local_min, '*', c='c', ms=10, label='local min: %.2f' % c_local_min)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"Non-ergodicity paramter\", fontdict=font_plt_ax)\n",
    "ax.legend(loc=0)\n",
    "\n",
    "#Plot the stretching exponent versus the wave vector\n",
    "ax = plt.subplot(3,1,3)\n",
    "plt.title(\"Stretching Exponent\")\n",
    "ax.semilogx(q[3:-1], alph_v2_varyalpha[2:-1], 'ro',label='stretching exp allowed to vary')\n",
    "#ax.semilogx(q[3:-1], alph_v2[2:-1], 'rs')\n",
    "ax.plot(qs[b[0]],alph_v2_varyalpha[2:-1][b[0]],'b+',label='region to find avg')\n",
    "ax.hlines(alph_v2_varyalpha[2:-1][b[0]].mean(), qs[2],qs[-1], linestyles='dashed')\n",
    "ax.text(0.4,0.5,\"Avg stretching exp: %.4f\" % alph_v2_varyalpha[2:-1][b[0]].mean())\n",
    "ax.text(0.4,0.12,\"Fixed stretching exp: %.4f\" % alph_v2[2])\n",
    "ax.set_ylim(0,3)\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"Stretching exponent\", fontdict=font_plt_ax)\n",
    "ax.legend(loc=0)\n",
    "\n",
    "#Save this figure at a PNG file.\n",
    "plt.savefig(data_dir+data_file[:-18]+\"tauvsq_nonergparam_ExpoFit.png\",dpi=150)\n",
    "#print(\"Saved to %s\" % data_dir+data_file[:-23]+\"_tauvsq_nonergparam_ExpoFit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!\n",
    "\n",
    "After looking at the plots above, figure out a good value for the stretching exponent (what we call 'alpha' in the code). It should be the average stretching exponent over a range of q values. The range of q values in use is indicated with the red or blue crosses on the plot of the decay time vs q and stretching exponent vs q. If that range seems inappropriate, change the `minq` and `maxq` paratmers.\n",
    "\n",
    "Once you've found that, go back to the fitting function -- the `isf_fitting` function -- was defined, and find the first line of that block of code: <br />\n",
    "`STRETCHING_EXP = 0.6`. <br />\n",
    "(Or, it might not say '0.6' but some other number.) <br />\n",
    "Change that value from whatever is to the new value (probably something between 0.5 and 0.8 though it could range anywhere between 0 and 1) and you only need to go to the hundredths place -- no need to go to further decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_parameters = {} #initialize empty dictionary\n",
    "fitting_parameters['last_times'] = last_times\n",
    "fitting_parameters['qs'] = q\n",
    "fitting_parameters['q_used'] = [minq,maxq]\n",
    "fitting_parameters['c'] = c\n",
    "fitting_parameters['c_varyalpha'] = c_varyalpha\n",
    "fitting_parameters['c_localmax'] = c_local_max\n",
    "fitting_parameters['c_localmin'] = c_local_min\n",
    "fitting_parameters['tau'] = tau_v2\n",
    "fitting_parameters['tau_varyalpha'] = tau_v2_varyalpha\n",
    "fitting_parameters['stretching_exponent'] = alph_v2_varyalpha\n",
    "fitting_parameters['stretching_exponent_fixed'] = alph_v2[2]\n",
    "fitting_parameters['k'] = Dif\n",
    "fitting_parameters['alpha'] = alpha\n",
    "fitting_parameters['fps'] = fps\n",
    "fitting_parameters['pixel_size'] = pixel_size\n",
    "fitting_parameters['data_directory'] = data_dir\n",
    "fitting_parameters['Movie'] = movie_num\n",
    "fitting_parameters['ROI'] = ROI\n",
    "fitting_parameters['ISF'] = isf\n",
    "fitting_parameters['ISF_theory'] = theory_isf\n",
    "fitting_parameters['chi'] = chi2\n",
    "fitting_parameters['times'] = times\n",
    "fitting_parameters['background'] = background\n",
    "fitting_parameters['ravs'] = ravs\n",
    "\n",
    "data_file_p = data_file[:-18]+\"NormalizedISFFitting_ExpoFit.p\"\n",
    "f = open(data_dir + data_file_p,'wb')\n",
    "pickle.dump(fitting_parameters, f, protocol=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write some data to more readable file\n",
    "csv_data_file = condition+\"_\"+date+\"_%i_ROI_%i_256x256_parameters_ExpoFit.csv\" % (movie_num, ROI)\n",
    "f = open(data_dir + csv_data_file,'wb')\n",
    "filewriter = csv.writer(f, delimiter=',')\n",
    "filewriter.writerow([data_dir])\n",
    "filewriter.writerow(['Movie','ROI','k','alpha','slope','speed','avg stretching exp','non-erq at q=2.457'])\n",
    "filewriter.writerow([str(movie_num), str(ROI), \"%.4f\" % Dif, \"%.4f\" % alpha, \"%.4f\" % slope, \"%.4f\" % fix_speed, \n",
    "                     \"%.4f\" % alph_v2[2:-1][b[0]].mean(), \n",
    "                    \"%.4f\" % c[35]])\n",
    "f.close()\n",
    "\n",
    "print(\"Saving to a csv file that you can open with Excel: \\n\")\n",
    "print(\"  k: %.4f \\n  alpha: %.4f \\n  stretch. exp.: %.4f \\n  c[35]: %.4f\" % (Dif, alpha, alph_v2[2:-1][b[0]].mean(), c[35]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At this point, you've analyzed the ISF with a exponential function. If this was an unstimulated video, you're done! Go to the next ROI or video.\n",
    "\n",
    "# If not, continue on and analyze the ISFs by fitting an exponential+ballistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# Stretching exponent.\n",
    "# This number must be between 0 and 1. \n",
    "# And it will probably be around 0.8\n",
    "# When you specify the value, don't go beyond hundredths place.\n",
    "# Set this to the AVERAGE VALUE you find when you plot the set of 7 plots below\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def errorfunc_for_scipy_leastsq_fit(params, data, times):\n",
    "    theory = ddm.dTheoryTwoModeISF(times, params[0],params[1],params[2], params[3], params[4], params[5])\n",
    "    return data-theory\n",
    "\n",
    "def isf_fitting(data, times):\n",
    "    '''\n",
    "    This function does the ISF fitting. \n",
    "\n",
    "    We'll just use one round. Using the scipy least_squares module\n",
    "    '''\n",
    "    \n",
    "    #Our parameters are: c (non-ergodicity param), t1 (diffusive), stretching exponent, t2 (ballistic), \n",
    "                        #a (proportion of those moving ballistically), Z\n",
    "    pars = np.zeros(6)*1.0\n",
    "    minp = np.zeros_like(pars)\n",
    "    maxp = np.zeros_like(pars)\n",
    "    lmin = np.array([True, True, True, True, True, True])\n",
    "    lmax = np.array([True, True, True,True, True, True])\n",
    "    fix = np.array([False, False, False,False, False, False])\n",
    "    \n",
    "    #come up with limits:\n",
    "    minp[0] = 0.0 #minimum non-erg parameter\n",
    "    maxp[0] = 1.0 #maximum non-erg parameter\n",
    "    \n",
    "    minp[1] = 0.01 #minimum diffusive decay time\n",
    "    maxp[1] = 2000.0  #maximum diffusive decay time\n",
    "    \n",
    "    minp[2] = 0.2 #minimum stretching exponent\n",
    "    maxp[2] = 1.0 #maximum stretching exponent\n",
    "    \n",
    "    minp[3] = 0.01 #minimum ballistic decay time\n",
    "    maxp[3] = 2000.0 #maximum ballistic decay time\n",
    "    \n",
    "    minp[4] = 0.1 #minimum ballistic proportion\n",
    "    maxp[4] = 1.0  #maximum ballistic proportion\n",
    "    \n",
    "    minp[5] = 0.1 #minimum Schulz Z\n",
    "    maxp[5] = 10. #maximum Schulz Z\n",
    "    \n",
    "    #initial guesses and fixes\n",
    "    pars[0] = 0.0 #non-ergodicity parameter\n",
    "        \n",
    "    pars[1] = 50.0 #decay time\n",
    "    \n",
    "    pars[2] = 0.8 #stretching exponent\n",
    "    \n",
    "    pars[3] = 2.0 #ballistic decay time \n",
    "    \n",
    "    pars[4] = 0.8 #proportion of population\n",
    "    \n",
    "    pars[5] = 3.0 #schulz number\n",
    "    \n",
    "    # First step, use the Scipy Least Squares function to find best parameters\n",
    "    fitparams_lstsq_temp = least_squares(errorfunc_for_scipy_leastsq_fit, pars, bounds=(minp,maxp), loss=\"linear\",args=(data,times))\n",
    "    fitparams_isf_lstsq = fitparams_lstsq_temp.x\n",
    "    error = fitparams_lstsq_temp.fun\n",
    "    theory_isf_lstsq = ddm.dTheoryTwoModeISF(times, fitparams_isf_lstsq[0], fitparams_isf_lstsq[1], \n",
    "                                             fitparams_isf_lstsq[2],fitparams_isf_lstsq[3], fitparams_isf_lstsq[4], \n",
    "                                             fitparams_isf_lstsq[5])\n",
    "    \n",
    "    return fitparams_isf_lstsq, theory_isf_lstsq, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# We don't have to fit all time lags. The long time lags may be noisy.\n",
    "#########################################################################\n",
    "# not implemented anymore: last_time = -350 # ONLY FIT UP TO THIS FINAL TIME POINT\n",
    "\n",
    "plt.figure(figsize=(10,12)) #Create figure of size 10x12\n",
    "\n",
    "\n",
    "#Loop over 8 different q-values to plot the ISF\n",
    "for i,q_index in enumerate([10,15,20,25,30,40,50,60]):\n",
    "    \n",
    "    last_times = np.linspace(450,2,num=len(q),dtype=np.int)\n",
    "    last_time = last_times[q_index]\n",
    "    #Here is where we override the variable end time fit, as noted above\n",
    "    last_time = 400\n",
    "    \n",
    "    fp_isf_lstsq, theory_isf_lstsq, fit_error = isf_fitting(isf[:last_time,q_index],times[:last_time])\n",
    "    full_time_theory = ddm.dTheoryTwoModeISF(times, *fp_isf_lstsq)\n",
    "\n",
    "    ax = plt.subplot(4,2,i+1) #creating 4 subplots in a 2x2 grid\n",
    "    ax.semilogx(times[:],isf[:,q_index],'ro',alpha=0.8)\n",
    "    #ax.plot(times[:last_time], theory_isf, '-b',lw=3,alpha=0.5) #BLUE LINE: Leven-Marq fitting method\n",
    "    ax.plot(times, full_time_theory, '--k',lw=1,alpha=0.8) #BLUE LINE: Leven-Marq fitting method\n",
    "    ax.plot(times[:last_time], theory_isf_lstsq,'-g',lw=3,alpha=0.5) #GREEN LINE: scipy.optimize's leastsquares function\n",
    "    \n",
    "    ax.text(0.05,0.7, \"diffusive decay time: %.2f\" % (fp_isf_lstsq[1]), fontdict=font_plt_ax)\n",
    "    ax.text(0.05,0.6, \"non-erg param: %.2f\" % (fp_isf_lstsq[0]), fontdict=font_plt_ax)\n",
    "    ax.text(0.05,0.5, \"stretch exp: %.2f\" % (fp_isf_lstsq[2]), fontdict=font_plt_ax)\n",
    "    ax.text(0.05,0.4, \"ballistic decay time: %.2f\" % (fp_isf_lstsq[3]), fontdict=font_plt_ax)\n",
    "    ax.text(0.05,0.3, \"a proportion: %.2f\" % (fp_isf_lstsq[4]), fontdict=font_plt_ax)\n",
    "    ax.text(0.05,0.2, \"schulz: %.2f\" % (fp_isf_lstsq[5]), fontdict=font_plt_ax)  \n",
    "    \n",
    "    ax.set_xlabel(\"Time (s)\", fontdict=font_plt_ax)\n",
    "    ax.set_title(\"Fit for q-index of %i. So q = %.3f 1/$\\mu$m\" % (q_index, q[q_index]), fontdict=font_plt_ax)\n",
    "    ax.set_ylim(-0.05,1.05)\n",
    "    \n",
    "plt.savefig(data_dir+data_file[:-18]+\"ISF_fits_TwoModeFit.png\",dpi=150)\n",
    "print(\"Saved as %s\" % data_dir+data_file[:-18]+\"ISF_fits_TwoModeFit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the above fits to the normalize image structure function look okay. If not, you can try adjusting the `last_times` parameter. Sometimes, removing more of the last few time points from the data we fit to helps since the data associated with very long time lags tends to be noisier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we do the fits for each wave vector (each q). We do this twice. One with fixing the stretching exponent and one time with letting it vary. When you see the results after the following code block, you'll choose the value for this stretching exponent and insert that value back into the block of code where the function `isf_fitting` was defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we'll do that fit for *all* q-values\n",
    "start=timer()\n",
    "#Initialize arrays to store the fit parameters\n",
    "tau_diff = np.zeros_like(ravs[0,:]) #diff decay time\n",
    "c = np.zeros_like(tau_diff) #this is the non-ergodicity parameter\n",
    "alph_v2 = np.zeros_like(tau_diff) #alpha (stretching exponent)\n",
    "tau_ball = np.zeros_like(tau_diff) #ballistic decay time\n",
    "a_pop = np.zeros_like(tau_diff) #population proportion\n",
    "z_s = np.zeros_like(tau_diff) #schulz #\n",
    "fit_error_rms = np.zeros_like(tau_diff)\n",
    "\n",
    "for i in range(1,len(tau_diff)):\n",
    "    fp_isf_lstsq, theory_isf_lstsq, error = isf_fitting(isf[:last_time,i],times[:last_time])\n",
    "    c[i] = fp_isf_lstsq[0]\n",
    "    tau_diff[i] = fp_isf_lstsq[1]\n",
    "    alph_v2[i] = fp_isf_lstsq[2]\n",
    "    tau_ball[i] = fp_isf_lstsq[3]\n",
    "    a_pop[i] = fp_isf_lstsq[4]\n",
    "    z_s[i] = fp_isf_lstsq[5]\n",
    "    fit_error_rms[i] = np.mean(np.sqrt(error**2))\n",
    "end=timer()\n",
    "print 'seconds elapsed: %.2f' %(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to fit some q vs tau plots!\n",
    "\n",
    "There are 7 plots here:\n",
    "\n",
    "1: The diffusive decay time v tau. You'll see a **minqd** and **maxqd** in the next cell. These values dictate the q range you try to fit. We'll also throw on a line with a slope of -2 (diffusive, anomalous scaling exponent=1). Hopefully there is a      region where the tau vs q scales diffusively or subdiffusively, slope < -2 (i.e. -3), *fit that*\n",
    "\n",
    "2: The ballistic decay time v tau. You'll see a **minqb** and **maxqb** in the next cell. These values dictate the q range you try to fit. We'll also throw on a line with a slope of -1 (ballistic). Hopefully there is a region where the tau vs q          scales, *fit that*\n",
    "\n",
    "3: Both decay times on the same plot\n",
    "\n",
    "4: Non-ergodicity parameter vs q (c vs q). If our ISF's do not decay to zero, the diffusion is **not** ergodic.\n",
    "\n",
    "5: Stretching exponent vs q (alph_v2 vs q). If the averages are way off your guess, you should your guess above. If the         stretching exponent is crashing into your lower bound, you can change that, too. Keep this parameter between 0-1, though.\n",
    "\n",
    "6: Proportion of the population that is ballistic (a_pop vs q). This should be nearly one for the qs we end up fitting, but has to be between 0-1. Our equation is ~ (diffusive term)((1-apop)+a_pop(ballistic term)). When a_pop is one, everything is        diffusing ballistically. When a_pop is zero, we have no ballistic motion.\n",
    "\n",
    "7: Schulz parameter vs q (z_s vs q). This parameter is tied into the variance of the velocity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,7*8/1.618)); ax = fig.gca();\n",
    "\n",
    "\n",
    "qs=q[3:-1]\n",
    "new_tau_diff=ddm.newt(tau_diff[2:-1],alph_v2[2:-1])\n",
    "new_tau_ball = tau_ball[2:-1]\n",
    "#new_tau_ball=ddm.newt(tau_ball[2:-1],alph_v2[2:-1])\n",
    "######################################################\n",
    "#Plot the diffusive decay time versus the wave vector#\n",
    "######################################################\n",
    "\n",
    "ax = plt.subplot(7,1,1)\n",
    "plt.title(\"Diffusive Decay time vs q\")\n",
    "ax.loglog(qs, new_tau_diff,'b.', label='diffusive taus')\n",
    "#############################################################################\n",
    "# Pick the range of q-value that seem to fit best (usually ~25ish to ~42ish)\n",
    "#############################################################################\n",
    "\n",
    "minqd=3\n",
    "maxqd=12\n",
    "qmin=qs[minqd]\n",
    "qmax=qs[maxqd]\n",
    "bd = np.where((qs>=qmin)&(qs<=qmax))\n",
    "\n",
    "ax.plot(qs[bd],new_tau_diff[bd],'r+',label='good q range')\n",
    "a = np.polyfit(np.log(qs[bd]),np.log(new_tau_diff[bd]), 1)\n",
    "slope_d = a[0]\n",
    "coef1_d = np.exp(a[1])\n",
    "alpha_d = 2./(-1*slope_d)\n",
    "Dif_d = (1.0/coef1_d)**alpha_d\n",
    "tau_fit = coef1_d*(qs**(-2.0/alpha_d))\n",
    "#speed = (1.0/coef1)\n",
    "ax.plot(qs, tau_fit, '-k')\n",
    "\n",
    "## does it fit some other power?\n",
    "fix_speed_d= .005\n",
    "fix_slope_d = 2.\n",
    "ax.plot(qs[:-20], (1./fix_speed_d) * (1./(qs[:-20]**(fix_slope_d))), '--m', label=\"diffusive, $\\ t$$^{-2}$\")\n",
    "\n",
    "#print(\"'slope' of tau vs q is %.2f\" % slope)\n",
    "#print(\"'speed' of tau vs q is %.4f\" % speed)\n",
    "ax.text(0.4,4, \"alpha: %.4f\" % alpha_d)\n",
    "ax.text(0.4,2, \"k: %.4f\" % Dif_d)\n",
    "ax.text(0.4,10, \"%i < q < %i\" % (minqd, maxqd))\n",
    "#ax.text(0.9,2, \"slope: %.4f\" % (slope))\n",
    "#ax.text(0.4,1, \"speed: %.4f\" % (speed))\n",
    "#ax.text(0.9,1, \"fix_speed: %.4f\" % (fix_speed))\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"tau (s)\", fontdict=font_plt_ax)\n",
    "ax.set_ylim(0.7,10000)\n",
    "#ax.set_xlim(0.3,1000)\n",
    "ax.legend(loc=1)\n",
    "\n",
    "######################################################\n",
    "#Plot the ballistic decay time versus the wave vector#\n",
    "######################################################\n",
    "ax = plt.subplot(7,1,2)\n",
    "plt.title(\"Ballistic Decay time vs q\")\n",
    "minqb=4\n",
    "maxqb=15\n",
    "qmin=qs[minqb]\n",
    "qmax=qs[maxqb]\n",
    "bb = np.where((qs>=qmin)&(qs<=qmax))\n",
    "ax.loglog(qs, new_tau_ball,'g.', label='ballistic taus')\n",
    "#ax.loglog(qs, tau_ball_varyalpha[2:-1],'gs',alpha=0.3, label='allowing str exp to vary')\n",
    "\n",
    "ax.plot(qs[bb],new_tau_ball[bb],'r+',label='good q range')\n",
    "a = np.polyfit(np.log(qs[bb]),np.log(new_tau_ball[bb[0]]), 1)\n",
    "slope_b = a[0]\n",
    "coef1_b = np.exp(a[1])\n",
    "alpha_b = 2./(-1*slope_b)\n",
    "speed = (1.0/coef1_b)\n",
    "#Dif = (1.0/coef1)**alpha\n",
    "tau_fit = coef1_b*(qs**(-2.0/alpha_b))\n",
    "ax.plot(qs, tau_fit, '-k')\n",
    "\n",
    "## does it fit some other power?\n",
    "fix_speed= .045\n",
    "fix_slope = 1.0\n",
    "ax.plot(qs[:-20], (1./fix_speed) * (1./(qs[:-20]**(fix_slope))), '--m', label=\"ballistic, $\\ t$$^{-1}$\")\n",
    "ax.set_ylim(0.7,1e3)\n",
    "#ax.set_xlim(0.3,1000)\n",
    "ax.legend(loc=1)\n",
    "\n",
    "#ax.text(0.4,4, \"alpha: %.4f\" % alpha)\n",
    "#ax.text(0.4,2, \"k: %.4f\" % Dif)\n",
    "ax.text(0.4,10, \"%i < q < %i\" % (minqb, maxqb))\n",
    "ax.text(0.4,5, \"slope: %.4f\" % (slope_b))\n",
    "ax.text(0.4,1, \"speed: %.4f\" % (speed))\n",
    "ax.text(0.4,3, \"fix_speed: %.4f $\\mu$m/s\" % (fix_speed))\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"tau (s)\", fontdict=font_plt_ax)\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"tau (s)\", fontdict=font_plt_ax)\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "#######Plot both decay times versus wave vector#######\n",
    "######################################################\n",
    "ax = plt.subplot(7,1,3)\n",
    "ax.loglog(qs, new_tau_diff,'b.', label='diffusive taus')\n",
    "\n",
    "ax.plot(qs[:-20], (1./fix_speed_d) * (1./(qs[:-20]**(fix_slope_d))), '--b', label=\"diffusive, $\\ t$$^{-2}$\")\n",
    "\n",
    "ax.loglog(qs, new_tau_ball,'g.', label='ballistic taus')\n",
    "\n",
    "ax.plot(qs[:-20], (1./fix_speed) * (1./(qs[:-20]**(fix_slope))), '--g', label=\"ballistic, $\\ t$$^{-1}$\")\n",
    "ax.legend()\n",
    "\n",
    "#Plot the non-erg parameter versus the wave vector\n",
    "ax = plt.subplot(7,1,4)\n",
    "plt.title(\"Non-ergodicity (fixed background to %.1f)\" % background)\n",
    "ax.semilogx(q[3:-1], c[2:-1], 'ro')\n",
    "\n",
    "ax.semilogx(qs[bd], c[2:-1][bd], 'b+',label='qs to fit tau diff')\n",
    "\n",
    "ax.semilogx(qs[bb], c[2:-1][bb], 'g+',label='qs to fit tau ball')\n",
    "\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"Non-ergodicity parameter\", fontdict=font_plt_ax)\n",
    "ax.legend(loc=0)\n",
    "\n",
    "\n",
    "#####################################################\n",
    "#Plot the stretching exponent versus the wave vector#\n",
    "#####################################################\n",
    "ax = plt.subplot(7,1,5)\n",
    "plt.title(\"Stretching Exponent\")\n",
    "ax.semilogx(q[3:-1], alph_v2[2:-1], 'ro',label='stretching exp allowed to vary')\n",
    "##Average Stretching Exponent of the region with diffusive tau fit\n",
    "ax.plot(qs[bd],alph_v2[2:-1][bd],'b+',label='diff. region to find avg')\n",
    "ax.hlines(alph_v2[2:-1][bd].mean(), qs[2],qs[-1], linestyles='dashed',colors=\"b\")\n",
    "ax.text(0.5,1.3,\"Avg diff. stretching exp: %.4f\" % alph_v2[2:-1][bd].mean())\n",
    "\n",
    "##Average Stretching Exponent of the region with ballistic tau fit\n",
    "ax.plot(qs[bb],alph_v2[2:-1][bb],'g+',label='Ball. region to find avg')\n",
    "ax.hlines(alph_v2[2:-1][bb].mean(), qs[2],qs[-1], linestyles='dashed', colors=\"g\")\n",
    "ax.text(0.5,1.2,\"Avg ball. stretching exp: %.4f\" % alph_v2[2:-1][bb].mean())\n",
    "\n",
    "ax.set_ylim(0,1.5)\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"Stretching exponent\", fontdict=font_plt_ax)\n",
    "ax.legend(loc=1)\n",
    "\n",
    "##########################################################\n",
    "#Plot the proportion of population versus the wave vector#\n",
    "##########################################################\n",
    "ax = plt.subplot(7,1,6)\n",
    "plt.title(\"Ballistic Proportion\")\n",
    "ax.semilogx(q[3:-1], a_pop[2:-1], 'ro',label=None)\n",
    "ax.plot(qs[bd],a_pop[2:-1][bd],'b+',label='qs to fit tau diff.')\n",
    "ax.plot(qs[bb],a_pop[2:-1][bb],'g+',label='qs to fit tau ball.')\n",
    "\n",
    "ax.set_ylim(-.1,1.1)\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"a. ballistic proportion\", fontdict=font_plt_ax)\n",
    "ax.legend(loc=0)\n",
    "\n",
    "###########################################\n",
    "#Plot the schulz parameter vs wave vector#\n",
    "###########################################\n",
    "ax = plt.subplot(7,1,7)\n",
    "plt.title(\"Schulz parameter, $\\sigma$$^{2}$ = $v$$^{2}$/(Z+1)\")\n",
    "ax.semilogx(q[3:-1], z_s[2:-1], 'ro',label=None)\n",
    "ax.plot(qs[bd],z_s[2:-1][bd],'b+',label='qs to fit tau diff.')\n",
    "ax.plot(qs[bb],z_s[2:-1][bb],'g+',label='qs to fit tau ball.')\n",
    "#ax.set_ylim(-.1,1.1)\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"Schulz Parameter, Z\", fontdict=font_plt_ax)\n",
    "ax.legend(loc=0)\n",
    "\n",
    "\n",
    "#Save this figure at a PNG file.\n",
    "plt.savefig(data_dir+data_file[:-18]+\"tauvsq_nonergparam.png\",dpi=150)\n",
    "print(\"Saved to %s\" % data_dir+data_file[:-18]+\"tauvsq_nonergparam_TwoModeFit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(q[1:], fit_error_rms,'ro')\n",
    "plt.xlabel(\"wave vector, q\")\n",
    "plt.ylabel(\"Root mean squared error\")\n",
    "plt.title(\"Error in Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!\n",
    "\n",
    "After looking at the plots above, figure out a good value for the stretching exponent (what we call 'alpha' in the code). It should be the average stretching exponent over a range of q values. The range of q values in use is indicated with the red or blue crosses on the plot of the decay time vs q and stretching exponent vs q. If that range seems inappropriate, change the `minq` and `maxq` parameters.\n",
    "\n",
    "Once you've found that, go back to the fitting function -- the `isf_fitting` function -- was defined, and find the first line of that block of code: <br />\n",
    "`STRETCHING_EXP = 0.6`. <br />\n",
    "(Or, it might not say '0.6' but some other number.) <br />\n",
    "Change that value from whatever is to the new value (probably something between 0.3 and 1.0 though it could range anywhere between 0 and 1) and you only need to go to the hundredths place -- no need to go to further decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure your datafile is correct for saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200924_75A25M_myosin_beads_405stim_561OD3_100ms_10fps_3_256_256x256_\n"
     ]
    }
   ],
   "source": [
    "print data_file[:-18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_parameters = {} #initialize empty dictionary\n",
    "fitting_parameters['last_time'] = last_time\n",
    "fitting_parameters['qs'] = q\n",
    "fitting_parameters['qb_used'] = [minqb,maxqb]\n",
    "fitting_parameters['qd_used'] = [minqd,maxqd]\n",
    "fitting_parameters['c'] = c \n",
    "fitting_parameters['tau_diff'] = tau_diff\n",
    "fitting_parameters['tau_ball'] = tau_ball\n",
    "fitting_parameters['stretching_exponent'] = alph_v2\n",
    "fitting_parameters['a_ballistic_prop'] = a_pop\n",
    "fitting_parameters['schulz'] = z_s\n",
    "fitting_parameters['fps'] = fps\n",
    "fitting_parameters['pixel_size'] = pixel_size\n",
    "fitting_parameters['data_directory'] = data_dir\n",
    "fitting_parameters['Movie'] = movie_num\n",
    "fitting_parameters['ROI'] = ROI\n",
    "fitting_parameters['ISF'] = isf\n",
    "fitting_parameters['ISF_theory'] = theory_isf_lstsq\n",
    "fitting_parameters['times'] = times\n",
    "fitting_parameters['background'] = background\n",
    "fitting_parameters['ravs'] = ravs\n",
    "fitting_parameters['error'] = fit_error_rms\n",
    "\n",
    "data_file_p = data_file[:-18]+\"NormalizedISFFitting_TwoModeFit.p\"\n",
    "f = open(data_dir + data_file_p,'wb')\n",
    "pickle.dump(fitting_parameters, f, protocol=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write some data to more readable file\n",
    "csv_data_file = condition+\"_\"+date+\"_%i_ROI_%i_256x256_parameters_TwoModeFit.csv\" % (movie_num, ROI)\n",
    "f = open(data_dir + csv_data_file,'wb')\n",
    "filewriter = csv.writer(f, delimiter=',')\n",
    "filewriter.writerow([data_dir])\n",
    "filewriter.writerow(['date','Movie','ROI','k','alpha','avg stretching exp diff',\n",
    "                     'slope','speed','fixed ballistic speed','avg stretching exp ball'])\n",
    "filewriter.writerow([str(date),str(movie_num), str(ROI), \"%.4f\" % Dif_d, \"%.4f\" % alpha_d, \"%.4f\" % alph_v2[2:-1][bd].mean(), \n",
    "                    \"%.4f\" % slope_b, \"%.4f\" % speed,\"%.4f\" % fix_speed,\"%.4f\" % alph_v2[2:-1][bb].mean()])\n",
    "f.close()\n",
    "\n",
    "print(\"Saving to a csv file that you can open with Excel \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After analyzing all four ROIs of a movie, combine data. Or skpi this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to combine these files from some other data directory, you can edit data_dir. Otherwise don't.\n",
    "print(\"Current 'data_dir' is %s\" % data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing data_dir\n",
    "#data_dir = \"D:\\\\Data\\\\Sylas\\\\2019-2020 XL Bead Analysis\\\\20_1_14_CoXL\\\\analysis\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(data_dir + \"*_%i_ROI*.csv\" % movie_num)\n",
    "\n",
    "csv_all_data_file = condition+\"_%i_all_256x256_parameters.csv\" % movie_num\n",
    "f_all = open(data_dir + csv_all_data_file,'wb')\n",
    "filewriter = csv.writer(f_all, delimiter=',')\n",
    "f_all_header_not_written_yet = True\n",
    "\n",
    "for i,filename in enumerate(csv_files):\n",
    "    if not \"_all_\" in filename:\n",
    "        f = open(filename,'r')\n",
    "        filereader = csv.reader(f, delimiter=',')\n",
    "        for j,row in enumerate(filereader):\n",
    "            if f_all_header_not_written_yet:\n",
    "                filewriter.writerow(row)\n",
    "            else:\n",
    "                if j>1:\n",
    "                    filewriter.writerow(row)\n",
    "        f_all_header_not_written_yet = False\n",
    "        f.close()\n",
    "f_all.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "widgets": {
   "state": {
    "0689152f16ed41dc8a8bcb723dd208ff": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "26aa4e49158c4ac1849007468d752942": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "2f4331df93e34013bb2c39527f242747": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "44268e92f7a341398441a40371d4d52e": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "865c7b14e4224de59bdedc2a153c180c": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "875f47d176124605be1a53d9c460194d": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "a7696b918cb04f9dba715f91419757d3": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "a7856b7a541d49899761758860c70a33": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "f3ec44c7464c43d6bb1b7c50fc36cceb": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "f848a825e63a484f96afe2eaf70e7127": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
