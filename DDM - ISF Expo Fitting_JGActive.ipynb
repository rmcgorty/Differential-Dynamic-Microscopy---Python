{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll try analyzing Sylas's data of beads in networks using methods described in this [Cho et al 2020 PRL paper](https://link.aps.org/doi/10.1103/PhysRevLett.124.088005).\n",
    "\n",
    "With DDM, we take a movie and generate the DDM matrix, $D(q,\\Delta t)$ (the generation of this DDM matrix should already have been done before going through this code). This can be fit to the function:\n",
    "$D(q,\\Delta t) = A(q)(1 - f(q,\\Delta t)) + B(q)$. \n",
    "The function $f(q,\\Delta t)$ is called the intermediate scattering function (ISF). And we usually assume it has the form: $f(q,\\Delta t) = \\mbox{exp}(-\\Delta t / \\tau (q))^{\\alpha (q)}$ where $\\alpha (q)$ is the stretching exponent and $\\tau (q)$ is the characteristic decay time.\n",
    "\n",
    "What we have usually tried is to take $D(q,\\Delta t)$ and, for each wave vector $q$, fit it to find the parameters $A, B, \\tau, \\text{ and } \\alpha$.\n",
    "\n",
    "What we do now (using the methods of [Cho et al](https://link.aps.org/doi/10.1103/PhysRevLett.124.088005)) is get the parameters $A$ and $B$ from the images themselves. Then we can get the ISF: $f(q,\\Delta t) = 1 - \\frac{D(q,\\Delta t) - B(q)}{A(q)}.$ \n",
    "\n",
    "We also add a new paramter to the ISF: the non-ergodicity parameter, $C$. So now we have that the ISF is equal to: $f(q,\\Delta t) = (1-C(q))\\mbox{exp}(-\\Delta t / \\tau (q))^{\\alpha (q)} + C(q)$. If $C$ is zero, then this ISF is just $\\mbox{exp}(-\\Delta t / \\tau (q))^{\\alpha (q)}$, as we had before. And that's the expected case for ergodic dynamics. But if the system is non-ergodic, then we expect a non-zero $C$, somewhere between 0 and 1. \n",
    "\n",
    "Note that in this code, we refer to $D(q, \\Delta t)$ as 'ravs'. That is because getting $D(q, \\Delta t)$ invovles finding the <b>r</b>adial <b>av</b>erages of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiff_file.py:1995: UserWarning: failed to import _tifffile.decodepackbits\n",
      "  warnings.warn(\"failed to import %s\" % module_function)\n",
      "tiff_file.py:1995: UserWarning: failed to import _tifffile.decodelzw\n",
      "  warnings.warn(\"failed to import %s\" % module_function)\n",
      "tiff_file.py:1995: UserWarning: failed to import _tifffile.unpackints\n",
      "  warnings.warn(\"failed to import %s\" % module_function)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "font_plt = {'family': 'serif','color':  'darkred','weight': 'normal','size': 8,}\n",
    "font_plt_ax = {'family': 'serif','color':  'black','weight': 'normal', 'size': 8,}\n",
    "\n",
    "import numpy as np #numerical python used for working with arrays, mathematical operations\n",
    "import time #useful for timing functions\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import glob #glob is helpful for searching for filenames or directories\n",
    "import ddm_clean as ddm #this is the module containing the differential dynamic microscopy code\n",
    "import scipy #scientific python\n",
    "from scipy.signal import blackmanharris as bh #for Blackman-Harris windowing\n",
    "from scipy.optimize import leastsq\n",
    "import pickle #for saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ddm_clean' from 'ddm_clean.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ddm) #reload of ddm necessary if changes have been made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie number and ROI specified below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#Specify where the data is stored and the image data filename. Must be in tiff format\n",
    "#######################################################################################\n",
    "\n",
    "########################\n",
    "# Select Movie Number  #\n",
    "movie_num = 1\n",
    "########################\n",
    "date= '2020-07-29'\n",
    "#######################\n",
    "condition=\"Actin_myosin\"\n",
    "#condition=\"AMT_myosin\"\n",
    "#condition=\"100M_myosin\"\n",
    "#condition=\"75A25M\"\n",
    "#condition=\"25A75M\"\n",
    "########### Select ROI here ###############\n",
    "ROI = 0  # <---- select ROI (0,256,512, or 768)\n",
    "###########################################\n",
    "\n",
    "data_dir = \"Y:\\\\Jon_Garamella\\\\data\\\\active_networks\\\\videos_date\\\\%s\\\\20200922_%s_594beads_25X_561OD3_50msEXPO_20fps_%s\\\\\" % (date,condition,movie_num)\n",
    "data_file = \"20200922_AMT_594beads_25X_561OD3_50msEXPO_20fps_%s_MMStack_Pos0.ome.tif\" %movie_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to look at the images, uncomment this out, otherwise we won't need to load the images or generate the ffts to get A&B as it's already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiff_file.py:725: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for p in pages)\n"
     ]
    }
   ],
   "source": [
    "#Image read using tiff_file module\n",
    "#im = ddm.tiff_file.imread(data_dir+data_file)\n",
    "\n",
    "#Display the second frame of the image\n",
    "#plt.figure(figsize=(8,3))\n",
    "#plt.matshow(im[1], cmap=matplotlib.cm.gray, fignum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open up the previosly generated DDM matrix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#  Specify where the DDM matrix data is stored\n",
    "#######################################################################################\n",
    "data_dir = \"Y:\\\\Jon_Garamella\\\\data\\\\active_networks\\\\videos_date\\\\%s\\\\ddm_analysis\\\\\" %(date,condition)\n",
    "data_file = \"20200922_AMT_594beads_25X_561OD3_50msEXPO_20fps_2_MMStack_Pos0.ome_256_256x256_FFTDIFFS_dts_ravs.p\" #%(movie_num,ROI)\n",
    "\n",
    "f = open(data_dir + data_file,'rb')\n",
    "p_data = pickle.load(f)\n",
    "f.close()\n",
    "print(\"The 'keys' contained in this pickle'd dictionary are: \", p_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy over that data stored in the dictionary\n",
    "ravs = p_data['ravs']\n",
    "dts = p_data['dts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Specify the frame rate (fps) and pixel size\n",
    "##########################################################################\n",
    "\n",
    "fps = 10.0 #The frame rate the video data was recorded at. \n",
    "times = dts/fps #Create the list of delay times in units of seconds\n",
    "pixel_size = 0.194 #pixel size in microns\n",
    "numPixels = 256 #number pof pixels in ROI\n",
    "q = np.arange(0,numPixels/2)*2*np.pi*(1./(numPixels*pixel_size)) #Convert the spatial frequencies to wave vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here, we'll show the image structure function for a particular q-value. \n",
    "\n",
    "qv=-1 # <-- this is the last q-value. useful for getting estimate of background\n",
    "fig = plt.figure(figsize=(6,6./1.618))\n",
    "plt.semilogx(times, ravs[:,qv],'ro')\n",
    "ax = plt.gca()\n",
    "plt.xlabel('Time (s)', fontdict=font_plt_ax, labelpad=-3);\n",
    "plt.title(\"D(q,dt) for q of %.2f $\\mu$m$^{-1}$\" % q[qv]);\n",
    "plt.hlines(ravs[0,qv], times[0], times[-1], linestyles='dashed')\n",
    "print(\"Horizontal line at %.1f\" % ravs[0,qv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we could try fitting $D(q, \\Delta t)$ to the model described at the very beginning to determine $A$, $B$, $\\tau$ and $\\alpha$. If you want to do that, you'll find the code at the end of this notebook. But that step isn't necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## New method for getting A and B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data file to store avg abs(fft)^2 which we need for getting A and B\n",
    "#First, let us check whether this file already exists. If it does,\n",
    "#we don't have to calculate it again\n",
    "data_file = \"20200729_Actin_0237myosin_561OD3_50msEXPO_20fps_%i_%i_256X256_imageffts_for_AB.p\" %(movie_num,ROI) \n",
    "\n",
    "data_file_exists = os.path.isfile(data_dir + data_file)\n",
    "if data_file_exists:\n",
    "    print(\"Data file already exists.\")\n",
    "else:\n",
    "    print(\"Data file does NOT exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This uses the variable 'ROI' which you should have set at the beginning (as 0, 256, 512, or 768)\n",
    "ROI = 256\n",
    "\n",
    "loc = 1 #0 is ROI 0, 1 is 256, 2 is 512, 3 is 768\n",
    "\n",
    "if not data_file_exists:\n",
    "    \n",
    "    #these are the initial and final x and y coordinates for \n",
    "    #  the different ROIs\n",
    "    xi=[5, 5, 5, 5]\n",
    "    xf=[261, 261, 261, 261]\n",
    "    yi=[0, 256, 512, 768]\n",
    "    yf=[256, 512, 768, 1024]\n",
    "    \n",
    "    #Get the dimensions of the ROI'd image\n",
    "    nframes, ndx, ndy = im[:,xi[loc]:xf[loc],yi[loc]:yf[loc]].shape\n",
    "    print(\"Our image has %i frames and each frame is %i x %i pixels\" % (nframes, ndx,ndy))\n",
    "\n",
    "    #get the average abs(fft)^2 for each frame\n",
    "    av_fftsq_of_each_frame = np.zeros_like(im[0,xi[loc]:xf[loc],yi[loc]:yf[loc]]*1.0) #initialize array\n",
    "    for i in range(0, nframes):\n",
    "        #looping over all frames in the movie\n",
    "        fft_of_image = np.fft.fft2(im[i,xi[loc]:xf[loc],yi[loc]:yf[loc]]*1.0)\n",
    "        sqr_of_fft = np.fft.fftshift(fft_of_image*np.conj(fft_of_image))\n",
    "        av_fftsq_of_each_frame = av_fftsq_of_each_frame + abs(sqr_of_fft)\n",
    "    av_fftsq_of_each_frame = av_fftsq_of_each_frame/(1.0*nframes*ndx*ndy)\n",
    "    \n",
    "    #get radially average\n",
    "    rad_av_av_fftsq = ddm.radialAvFFTs_v2(av_fftsq_of_each_frame.reshape(1,ndx,ndy))\n",
    "\n",
    "    data_file = \"%s\" %(data_file)\n",
    "    f = open(data_dir + data_file,'wb')\n",
    "    pickle.dump({'rad_av_av_fftsq': rad_av_av_fftsq}, f)\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open(data_dir + data_file,'rb')\n",
    "    p_data = pickle.load(f)\n",
    "    f.close()\n",
    "    rad_av_av_fftsq = p_data['rad_av_av_fftsq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not data_file_exists:    \n",
    "    #here we just show the fourier transformed image, if we had to calculate it\n",
    "    plt.figure()\n",
    "    plt.matshow(np.log(av_fftsq_of_each_frame),fignum=0)\n",
    "else:\n",
    "    print(\"The 2D array of the avg ft(I)^2 not saved, just the radial average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(q[3:], rad_av_av_fftsq[0,2:],'ro')\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"0.5 * (A+B)\")\n",
    "plt.hlines(rad_av_av_fftsq[0,-1], q[3], q[-1], linestyles='dashed')\n",
    "plt.title(\"Dashed horizontal line at %.2f\" % rad_av_av_fftsq[0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of the DDM data, I'd say the background is ~ 40 (in many cases, depends on video). \n",
    "Based off the above plot, seems like it (1/2)(A+B) is plateauing at high q to around ??. So B is around twice that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#You can play around with this 'background' parameter\n",
    "############################################################\n",
    "background = 720\n",
    "new_amplitude = (2*rad_av_av_fftsq[0]) - background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(q[3:], new_amplitude[2:], 'ro', label='Amplitude from new method')\n",
    "try: plt.plot(q[3:], amp[2:], 'mo', label='Amplitude from fitting method')\n",
    "except: print(\"'amp' not defined\")\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"amplitude\")\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our new value for amplitude, let's find the intermediate scattering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just double checking sizes of arrays:\n",
    "print(\"size of ravs array (the ddm matrix or image struct func): %i by %i\" % ravs.shape)\n",
    "print(\"size of new amplitudes: %i\" % new_amplitude.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize arrays to store the ISF (intermediate scattering function)\n",
    "isf = np.zeros_like(ravs) #decay time\n",
    "\n",
    "for i in range(1,ravs.shape[1]):\n",
    "    isf[:,i] = 1 - ((ravs[:,i] - background) / new_amplitude[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10)) #Create figure of size 15x15 (inches)\n",
    "\n",
    "\n",
    "#Loop over 8 different q-values to plot the ISF\n",
    "for i,q_index in enumerate([5,10,15,20,25,30,35,45]):\n",
    "\n",
    "    ax = plt.subplot(4,2,i+1) #creating 4 subplots in a 2x2 grid\n",
    "    ax.semilogx(times,isf[:,q_index],'ro',alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel(\"Time (s)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "    ax.set_title(\"Fit for q-index of %i. So q = %.3f 1/$\\mu$m\" % (q_index, q[q_index]), fontdict=font_plt_ax)\n",
    "    \n",
    "    ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the fitting function\n",
    "\n",
    "The fitting function, `isf_fitting`, is described below. After running through this cell (and the next 3 or 4), you'll need to come back to this point and re-run this function after you settle on a stretching exponent to use. You'll come back to the first uncommented-out line: <br>\n",
    "`STRETCHING_EXP = 0.6` <br>\n",
    "and set the value (which is the stretching exponent, $\\alpha (q)$) to the correct value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# Stretching exponent.\n",
    "# This number must be between 0 and 1. \n",
    "# And it will probably be around 0.6.\n",
    "# When you specify the value, don't go beyond hundredths place.\n",
    "# Set this to the AVERAGE VALUE you found when this parameter was allowed to vary.\n",
    "##################################################################################################\n",
    "STRETCHING_EXP = 1.0  # <----- THIS NUMBER NEEDS TO CHANGE AFTER GOING THROUGH NEXT COUPLE STEPS!\n",
    "\n",
    "##################################################################################################\n",
    "# Fix stretching exponent... it may help to fix\n",
    "# Set this to true or false. \n",
    "# \n",
    "# BUT WE'LL SET THIS PARAMETER'S VALUE (TRUE or FALSE) ELSEWHERE\n",
    "# So leave this commented out for now\n",
    "#  \n",
    "##################################################################################################\n",
    "FIX_STRETCHING_EXP = False #False will allow stretching exponent to vary. True will hold constant\n",
    "\n",
    "def errorfunc_for_scipy_leastsq_fit(params, data, times):\n",
    "    theory = ddm.dTheoryNonErgISF(times, params[0],params[1],params[2])\n",
    "    return data-theory\n",
    "\n",
    "def isf_fitting(data, times):\n",
    "    '''\n",
    "    This function does the ISF fitting. \n",
    "\n",
    "    We'll just use one round. Using Levenberg-Marquardt method with the mpfit module. \n",
    "    '''\n",
    "    \n",
    "    #Our parameters are: c (non-ergodicity param), tau, stretching epxonent\n",
    "    pars = np.zeros(3)*1.0\n",
    "    minp = np.zeros_like(pars)\n",
    "    maxp = np.zeros_like(pars)\n",
    "    lmin = np.array([True, True, True])\n",
    "    lmax = np.array([True, True, True])\n",
    "    fix = np.array([False, False, False])\n",
    "    \n",
    "    #come up with limits:\n",
    "    minp[0] = 0.0 #minimum non-erg parameter\n",
    "    maxp[0] = 1.0 #maximum non-erg parameter\n",
    "    minp[1] = 0.01 #minimum decay time\n",
    "    maxp[1] = 3000.0  #maximum decay time\n",
    "    minp[2] = 0.1 #minimum stretching exponent\n",
    "    maxp[2] = 1.0 #maximum stretching exponent\n",
    "    \n",
    "    #initial guesses \n",
    "    pars[0] = 0.25 #non-ergodicity parameter\n",
    "    pars[1] = 50.0 #decay time\n",
    "    ############################################################################\n",
    "    # Below (pars[2]) is the stretching exponent. \n",
    "    # After letting it vary, set to average value and\n",
    "    # fix it at that.\n",
    "    #\n",
    "    # YOU MUST CHANGE THIS TO AVERAGE VALUE OVER REASONALBE RANGE OF Q\n",
    "    ###########################################################################\n",
    "    pars[2] = STRETCHING_EXP #stretching exponent. \n",
    "    \n",
    "    fix[2] = FIX_STRETCHING_EXP   #True or False -- set above\n",
    "    \n",
    "    # First step, use the Scipy Least Squares function to find best parameters\n",
    "    #   We will then use those parameters as initial guess in the Levenberg-Marquardt method\n",
    "    fitparams_lstsq_temp = leastsq(errorfunc_for_scipy_leastsq_fit, pars, args=(data,times))\n",
    "    fitparams_isf_lstsq = fitparams_lstsq_temp[0]\n",
    "    theory_isf_lstsq = ddm.dTheoryNonErgISF(times, fitparams_isf_lstsq[0], fitparams_isf_lstsq[1], fitparams_isf_lstsq[2])\n",
    "    \n",
    "    #sometimes the leastsq's function will return parameters outside the limits we impose\n",
    "    #  so check for that and correct if necessary\n",
    "    for i in [1,2]:\n",
    "        if not fix[i]:\n",
    "            if fitparams_isf_lstsq[i] > maxp[i]:\n",
    "                pars[i] = 0.99*maxp[i]\n",
    "            elif fitparams_isf_lstsq[i] < minp[i]:\n",
    "                pars[i] = 1.01*minp[i]\n",
    "            else:\n",
    "                pars[i] = fitparams_isf_lstsq[i]\n",
    "        if fitparams_isf_lstsq[0]>0:\n",
    "            if fitparams_isf_lstsq[0]<1.0:\n",
    "                pars[0] = fitparams_isf_lstsq[0]\n",
    "\n",
    "\n",
    "    fitparams_isf, theory_isf, errCode, chi2 = ddm.newFit_ISF(data,times,pars,minp,maxp,lmin,lmax,fix,\n",
    "                                                              logfit=False,quiet=True,factor=1)\n",
    "    \n",
    "    return fitparams_isf, theory_isf, chi2, fitparams_isf_lstsq, theory_isf_lstsq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we define the time lag that we end the fits at. We do this because the data for long time lags gets noisier. It also gets more noisy for high q at long times than low q at long times. So we make the last time we fit to a function of q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You'll need to change that first number in the linspace function if you use the variable times\n",
    "last_times = np.linspace(400,2,num=len(q),dtype=np.int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we'll inspect some (8) of the fits to the ISF. Hopefully they look okay. Make a note if any of them look off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# We don't have to fit all time lags. The long time lags may be noisy.\n",
    "#########################################################################\n",
    "# not implemented anymore: last_time = -350 # ONLY FIT UP TO THIS FINAL TIME POINT\n",
    "\n",
    "plt.figure(figsize=(10,12)) #Create figure of size 10x12\n",
    "\n",
    "#########################################################################\n",
    "# In making these plots, we'll not fix the stretching exponent.\n",
    "# But feel free to change this.\n",
    "#########################################################################\n",
    "FIX_STRETCHING_EXP = False\n",
    "\n",
    "\n",
    "#Loop over 8 different q-values to plot the ISF\n",
    "for i,q_index in enumerate([15,20,25,30,35,40,50,60]):\n",
    "    \n",
    "    last_time = last_times[q_index]\n",
    "    #last_time = 300\n",
    "    fp_isf, theory_isf, chi2, fp_isf_lstsq, theory_isf_lstsq = isf_fitting(isf[:last_time,q_index],times[:last_time])\n",
    "    full_time_theory = ddm.dTheoryNonErgISF(times, *fp_isf)\n",
    "\n",
    "    ax = plt.subplot(4,2,i+1) #creating 4 subplots in a 2x2 grid\n",
    "    ax.semilogx(times[:],isf[:,q_index],'ro',alpha=0.8)\n",
    "    ax.plot(times[:last_time], theory_isf, '-b',lw=3,alpha=0.5) #BLUE LINE: Leven-Marq fitting method\n",
    "    ax.plot(times, full_time_theory, '--k',lw=1,alpha=0.8) #BLUE LINE: Leven-Marq fitting method\n",
    "    ax.plot(times[:last_time], theory_isf_lstsq,'-g',lw=3,alpha=0.5) #GREEN LINE: scipy.optimize's leastsquares function\n",
    "    \n",
    "    ax.text(0.15,0.4, \"decay time: %.1f, %.1f\" % (fp_isf[1], fp_isf_lstsq[1]), fontdict=font_plt_ax)\n",
    "    ax.text(0.15,0.3, \"non-erg param: %.2f, %.2f\" % (fp_isf[0], fp_isf_lstsq[0]), fontdict=font_plt_ax)\n",
    "    ax.text(0.15,0.2, \"stretch exp: %.2f, %.2f\" % (fp_isf[2], fp_isf_lstsq[2]), fontdict=font_plt_ax)\n",
    "    \n",
    "    \n",
    "    ax.set_xlabel(\"Time (s)\", fontdict=font_plt_ax)\n",
    "    ax.set_title(\"Fit for q-index of %i. So q = %.3f 1/$\\mu$m\" % (q_index, q[q_index]), fontdict=font_plt_ax)\n",
    "    ax.set_ylim(-0.05,1.05)\n",
    "    \n",
    "plt.savefig(data_dir+data_file[:-19]+\"_ISF_fits_ExpoFit.png\",dpi=150)\n",
    "print(\"Saved as %s\" % data_dir+data_file[:-19]+\"_ISF_fits_ExpoFit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the above fits to the normalize image structure function look okay. If not, you can try adjusting the `last_times` parameter. Sometimes, removing more of the last few time points from the data we fit to helps since the data associated with very long time lags tends to be noisier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we do the fits for each wave vector (each q). We do this twice. One with fixing the stretching exponent and one time with letting it vary. When you see the results after the following code block, you'll choose the value for this stretching exponent and insert that value back into the block of code where the function `isf_fitting` was defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we'll do that fit for *all* q-values\n",
    "start=timer()\n",
    "\n",
    "FIX_STRETCHING_EXP = True\n",
    "#Initialize arrays to store the fit paramters\n",
    "tau_v2 = np.zeros_like(ravs[0,:]) #decay time\n",
    "c = np.zeros_like(tau_v2) #this is the non-ergodicity parameter\n",
    "alph_v2 = np.zeros_like(tau_v2) #alpha (stretching exponent)\n",
    "for i in range(1,len(tau_v2)):\n",
    "    last_time = last_times[i]\n",
    "    fp_isf, theory_isf, chi2, fp_isf_lstsq, theory_isf_lstsq = isf_fitting(isf[:last_time,i],times[:last_time])\n",
    "    c[i] = fp_isf[0]\n",
    "    tau_v2[i] = fp_isf[1]\n",
    "    alph_v2[i] = fp_isf[2]\n",
    "\n",
    "    \n",
    "FIX_STRETCHING_EXP = False\n",
    "#Initialize arrays to store the fit paramters -- THIS TIME FIXING ALPHA\n",
    "tau_v2_varyalpha = np.zeros_like(ravs[0,:]) #decay time\n",
    "c_varyalpha = np.zeros_like(tau_v2) #this is the non-ergodicity parameter\n",
    "alph_v2_varyalpha = np.zeros_like(tau_v2) #alpha (stretching exponent)\n",
    "for i in range(1,len(tau_v2_varyalpha)):\n",
    "    last_time = last_times[i]\n",
    "    fp_isf, theory_isf, chi2, fp_isf_lstsq, theory_isf_lstsq = isf_fitting(isf[:last_time,i],times[:last_time])\n",
    "    c_varyalpha[i] = fp_isf[0]\n",
    "    tau_v2_varyalpha[i] = fp_isf[1]\n",
    "    alph_v2_varyalpha[i] = fp_isf[2]\n",
    "\n",
    "end=timer()\n",
    "print 'seconds elapsed: %.2f' %(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot the decay time versus the wave vector\n",
    "fig = plt.figure(figsize=(8,3*8/1.618)); ax = fig.gca();\n",
    "ax = plt.subplot(3,1,1)\n",
    "\n",
    "plt.title(\"Tau vs q -- \" + data_dir.split('\\\\')[-4] + \"; Movie \" + str(movie_num) + \"; ROI \" + str(ROI))\n",
    "qs=q[3:-1]\n",
    "new_taus=ddm.newt(tau_v2[2:-1],alph_v2[2:-1])\n",
    "new_taus_varyalpha=ddm.newt(tau_v2_varyalpha[2:-1],alph_v2_varyalpha[2:-1])\n",
    "#ax.loglog(qs, tau_v2[2:-1],'g.',alpha=0.2) \n",
    "ax.loglog(qs, new_taus,'bo', label='with str exponent fixed')\n",
    "ax.loglog(qs, new_taus_varyalpha,'gs',alpha=0.3, label='allowing str exp to vary')\n",
    "\n",
    "#############################################################################\n",
    "# Pick the range of q-value that seem to fit best (usually ~25ish to ~42ish)\n",
    "#############################################################################\n",
    "minq=13\n",
    "maxq=35\n",
    "\n",
    "#######################################\n",
    "#  You can comment out a large block  #\n",
    "#  by highlighting it and using three #\n",
    "#  quotation marks '''                #\n",
    "#######################################\n",
    "\n",
    "#############################################################################\n",
    "##################### Fit with alpha fixed!!!!!!! ###########################\n",
    "#############################################################################\n",
    "qmin=qs[minq]\n",
    "qmax=qs[maxq]\n",
    "b = np.where((qs>=qmin)&(qs<=qmax))\n",
    "ax.plot(qs[b[0]],new_taus[b[0]],'r+',label='good q range')\n",
    "a = np.polyfit(np.log(qs[b[0]]),np.log(new_taus[b[0]]), 1)\n",
    "slope = a[0]\n",
    "coef1 = np.exp(a[1])\n",
    "alpha = 2./(-1*slope)\n",
    "Dif = (1.0/coef1)**alpha\n",
    "tau_fit = coef1*(qs**(-2.0/alpha))\n",
    "ax.plot(qs, tau_fit, '-k')\n",
    "\n",
    "#############################################################################\n",
    "##################### Fit with alpha varied!!!!!!! ###########################\n",
    "#############################################################################\n",
    "'''qmin=qs[minq]\n",
    "qmax=qs[maxq]\n",
    "b = np.where((qs>=qmin)&(qs<=qmax))\n",
    "ax.plot(qs[b[0]],new_taus_varyalpha[b[0]],'r+',label='good q range')\n",
    "a = np.polyfit(np.log(qs[b[0]]),np.log(new_taus_varyalpha[b[0]]), 1)\n",
    "slope = a[0]\n",
    "coef1 = np.exp(a[1])\n",
    "alpha = 2./(-1*slope)\n",
    "Dif = (1.0/coef1)**alpha\n",
    "tau_fit = coef1*(qs**(-2.0/alpha))\n",
    "ax.plot(qs, tau_fit, '-k')'''\n",
    "\n",
    "## does it fit ballistically?\n",
    "fix_speed = .01\n",
    "fix_slope = 2. #diffusive = 2, ballistic 1\n",
    "ax.plot(qs[:-20], (1./fix_speed) * (1./(qs[:-20]**(fix_slope))), '--m', label=\"diffusive, $\\ t$$^{-2}$\")\n",
    "\n",
    "ax.text(0.4,1, \"alpha: %.4f\" % alpha)\n",
    "ax.text(1,2, \"k (if subdiff.): %.4f\" % Dif)\n",
    "ax.text(0.4,4, \"%i < q < %i\" % (minq, maxq))\n",
    "ax.text(0.4,2, \"slope: %.4f\" % (slope))\n",
    "ax.text(1,1, \"speed (if ball.): %.4f\" % (fix_speed))\n",
    "        \n",
    "        \n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"tau (s)\", fontdict=font_plt_ax)\n",
    "ax.set_ylim(0.7,1000)\n",
    "ax.legend(loc=1)\n",
    "\n",
    "\n",
    "#Plot the non-erg parameter versus the wave vector\n",
    "ax = plt.subplot(3,1,2)\n",
    "plt.title(\"Non-ergodicity (fixed background to %.1f)\" % background)\n",
    "ax.semilogx(q[3:-1], c[2:-1], 'ro')\n",
    "ax.semilogx(q[3:-1], c_varyalpha[2:-1], 'rs', alpha=0.4)\n",
    "c_range = np.where((qs>=q[45])&(qs<=q[75]))\n",
    "ax.semilogx(qs[c_range], c[2:-1][c_range], 'k.',label='region to find local max')\n",
    "c_local_max = c[2:-1][c_range].max()\n",
    "where_local_max = np.argmax(c[2:-1][c_range])\n",
    "ax.semilogx(qs[c_range][where_local_max], c_local_max, '*', c='y', ms=10, label='local max: %.2f' % c_local_max)\n",
    "c_range = np.where((qs>=q[25])&(qs<=q[50]))\n",
    "ax.semilogx(qs[c_range], c[2:-1][c_range], 'b.',label='region to find local min')\n",
    "c_local_min = c[2:-1][c_range].min()\n",
    "where_local_min = np.argmin(c[2:-1][c_range])\n",
    "ax.semilogx(qs[c_range][where_local_min], c_local_min, '*', c='c', ms=10, label='local min: %.2f' % c_local_min)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"Non-ergodicity paramter\", fontdict=font_plt_ax)\n",
    "ax.legend(loc=0)\n",
    "\n",
    "#Plot the stretching exponent versus the wave vector\n",
    "ax = plt.subplot(3,1,3)\n",
    "plt.title(\"Stretching Exponent\")\n",
    "ax.semilogx(q[3:-1], alph_v2_varyalpha[2:-1], 'ro',label='stretching exp allowed to vary')\n",
    "#ax.semilogx(q[3:-1], alph_v2[2:-1], 'rs')\n",
    "ax.plot(qs[b[0]],alph_v2_varyalpha[2:-1][b[0]],'b+',label='region to find avg')\n",
    "ax.hlines(alph_v2_varyalpha[2:-1][b[0]].mean(), qs[2],qs[-1], linestyles='dashed')\n",
    "ax.text(0.4,0.5,\"Avg stretching exp: %.4f\" % alph_v2_varyalpha[2:-1][b[0]].mean())\n",
    "ax.text(0.4,0.12,\"Fixed stretching exp: %.4f\" % alph_v2[2])\n",
    "ax.set_ylim(0,3)\n",
    "ax.set_xlabel(\"q ($\\mu$m$^{-1}$)\", fontdict=font_plt_ax, labelpad=-5)\n",
    "ax.set_ylabel(\"Stretching exponent\", fontdict=font_plt_ax)\n",
    "ax.legend(loc=0)\n",
    "\n",
    "#Save this figure at a PNG file.\n",
    "plt.savefig(data_dir+data_file[:-23]+\"_tauvsq_nonergparam_ExpoFit.png\",dpi=150)\n",
    "#print(\"Saved to %s\" % data_dir+data_file[:-23]+\"_tauvsq_nonergparam_ExpoFit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!\n",
    "\n",
    "After looking at the plots above, figure out a good value for the stretching exponent (what we call 'alpha' in the code). It should be the average stretching exponent over a range of q values. The range of q values in use is indicated with the red or blue crosses on the plot of the decay time vs q and stretching exponent vs q. If that range seems inappropriate, change the `minq` and `maxq` paratmers.\n",
    "\n",
    "Once you've found that, go back to the fitting function -- the `isf_fitting` function -- was defined, and find the first line of that block of code: <br />\n",
    "`STRETCHING_EXP = 0.6`. <br />\n",
    "(Or, it might not say '0.6' but some other number.) <br />\n",
    "Change that value from whatever is to the new value (probably something between 0.5 and 0.8 though it could range anywhere between 0 and 1) and you only need to go to the hundredths place -- no need to go to further decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_parameters = {} #initialize empty dictionary\n",
    "fitting_parameters['last_times'] = last_times\n",
    "fitting_parameters['qs'] = q\n",
    "fitting_parameters['q_used'] = [minq,maxq]\n",
    "fitting_parameters['c'] = c\n",
    "fitting_parameters['c_varyalpha'] = c_varyalpha\n",
    "fitting_parameters['c_localmax'] = c_local_max\n",
    "fitting_parameters['c_localmin'] = c_local_min\n",
    "fitting_parameters['tau'] = tau_v2\n",
    "fitting_parameters['tau_varyalpha'] = tau_v2_varyalpha\n",
    "fitting_parameters['stretching_exponent'] = alph_v2_varyalpha\n",
    "fitting_parameters['stretching_exponent_fixed'] = alph_v2[2]\n",
    "fitting_parameters['k'] = Dif\n",
    "fitting_parameters['alpha'] = alpha\n",
    "fitting_parameters['fps'] = fps\n",
    "fitting_parameters['pixel_size'] = pixel_size\n",
    "fitting_parameters['data_directory'] = data_dir\n",
    "fitting_parameters['Movie'] = movie_num\n",
    "fitting_parameters['ROI'] = ROI\n",
    "fitting_parameters['ISF'] = isf\n",
    "fitting_parameters['ISF_theory'] = theory_isf\n",
    "fitting_parameters['chi'] = chi2\n",
    "fitting_parameters['times'] = times\n",
    "fitting_parameters['background'] = background\n",
    "fitting_parameters['ravs'] = ravs\n",
    "\n",
    "data_file_p = data_file[:-18]+\"_NormalizedISFFitting_ExpoFit.p\"\n",
    "f = open(data_dir + data_file_p,'wb')\n",
    "pickle.dump(fitting_parameters, f, protocol=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write some data to more readable file\n",
    "csv_data_file = condition+\"_\"+date+\"_%i_ROI_%i_256x256_parameters_ExpoFit.csv\" % (movie_num, ROI)\n",
    "f = open(data_dir + csv_data_file,'wb')\n",
    "filewriter = csv.writer(f, delimiter=',')\n",
    "filewriter.writerow([data_dir])\n",
    "filewriter.writerow(['Movie','ROI','k','alpha','slope','speed','avg stretching exp','non-erq at q=2.457'])\n",
    "filewriter.writerow([str(movie_num), str(ROI), \"%.4f\" % Dif, \"%.4f\" % alpha, \"%.4f\" % slope, \"%.4f\" % fix_speed, \n",
    "                     \"%.4f\" % alph_v2[2:-1][b[0]].mean(), \n",
    "                    \"%.4f\" % c[35]])\n",
    "f.close()\n",
    "\n",
    "print(\"Saving to a csv file that you can open with Excel: \\n\")\n",
    "print(\"  k: %.4f \\n  alpha: %.4f \\n  stretch. exp.: %.4f \\n  c[35]: %.4f\" % (Dif, alpha, alph_v2[2:-1][b[0]].mean(), c[35]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At this point, run a new movie or ROI. If you're finished, copy the data from that excel file to the google doc!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After analyzing all four ROIs of a movie, combine data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to combine these files from some other data directory, you can edit data_dir. Otherwise don't.\n",
    "print(\"Current 'data_dir' is %s\" % data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing data_dir\n",
    "#data_dir = \"D:\\\\Data\\\\Sylas\\\\2019-2020 XL Bead Analysis\\\\20_1_14_CoXL\\\\analysis\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(data_dir + \"*_%i_ROI*.csv\" % movie_num)\n",
    "\n",
    "csv_all_data_file = condition+\"_%i_all_256x256_parameters.csv\" % movie_num\n",
    "f_all = open(data_dir + csv_all_data_file,'wb')\n",
    "filewriter = csv.writer(f_all, delimiter=',')\n",
    "f_all_header_not_written_yet = True\n",
    "\n",
    "for i,filename in enumerate(csv_files):\n",
    "    if not \"_all_\" in filename:\n",
    "        f = open(filename,'r')\n",
    "        filereader = csv.reader(f, delimiter=',')\n",
    "        for j,row in enumerate(filereader):\n",
    "            if f_all_header_not_written_yet:\n",
    "                filewriter.writerow(row)\n",
    "            else:\n",
    "                if j>1:\n",
    "                    filewriter.writerow(row)\n",
    "        f_all_header_not_written_yet = False\n",
    "        f.close()\n",
    "f_all.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "widgets": {
   "state": {
    "0689152f16ed41dc8a8bcb723dd208ff": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "26aa4e49158c4ac1849007468d752942": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "2f4331df93e34013bb2c39527f242747": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "44268e92f7a341398441a40371d4d52e": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "865c7b14e4224de59bdedc2a153c180c": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "875f47d176124605be1a53d9c460194d": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "a7696b918cb04f9dba715f91419757d3": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "a7856b7a541d49899761758860c70a33": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "f3ec44c7464c43d6bb1b7c50fc36cceb": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "f848a825e63a484f96afe2eaf70e7127": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
